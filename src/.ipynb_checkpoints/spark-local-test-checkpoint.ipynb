{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "import warnings\n",
    "import random\n",
    "import os\n",
    "\n",
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just created a SparkContext\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # we try to create a SparkContext to work locally on all cpus available\n",
    "    sc = ps.SparkContext('local[4]')\n",
    "    print(\"Just created a SparkContext\")\n",
    "except ValueError:\n",
    "    # give a warning if SparkContext already exists (for use inside pyspark)\n",
    "    warnings.warn(\"SparkContext already exists in this scope\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.3.17.29:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[4] appName=pyspark-shell>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heads = 4996900\n",
      "tails = 5003100\n",
      "ratio = 0.49969\n"
     ]
    }
   ],
   "source": [
    "n = 10000000\n",
    "heads = (sc.parallelize(range(n))\n",
    "    .map(lambda _: random.random())\n",
    "    .filter(lambda r: r < 0.5)\n",
    "    .count())\n",
    "    \n",
    "tails = n - heads\n",
    "ratio = heads / n\n",
    "\n",
    "print('heads =', heads)\n",
    "print('tails =', tails)\n",
    "print('ratio =', ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  is_prime(number):\n",
    "    factor_min = 2\n",
    "    factor_max = int(number ** 0.5) + 1\n",
    "    for factor in range(factor_min, factor_max):\n",
    "        if number % factor == 0:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97]\n"
     ]
    }
   ],
   "source": [
    "numbers = range(2, 100)\n",
    "\n",
    "primes = (sc.parallelize(numbers)\n",
    "    .filter(is_prime)\n",
    "    .collect())\n",
    "\n",
    "print(primes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = 's3a://mortar-example-data/airline-data'\n",
    "rdd = sc.textFile(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5113194"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('../data/referral/physician-shared-patient-patterns-2014-days180-sample.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+---+\n",
      "|       _c0|       _c1|       _c2|       _c3|_c4|\n",
      "+----------+----------+----------+----------+---+\n",
      "|1063590222|1760575641|96        |27        |  3|\n",
      "|1790184497|1427005735|68        |13        |  0|\n",
      "|1154381309|1861495335|253       |20        | 15|\n",
      "|1538144910|1801826219|40        |22        |  4|\n",
      "|1134293798|1346341948|68        |24        |  6|\n",
      "|1578678975|1720057714|540       |199       | 24|\n",
      "|1518905249|1952385098|111       |12        |  9|\n",
      "|1689976649|1982643235|67        |14        |  3|\n",
      "|1851355804|1164748810|209       |37        |  0|\n",
      "|1710971585|1821191149|475       |38        |  4|\n",
      "|1992735088|1336186972|78        |42        |  0|\n",
      "|1902072143|1386651297|179       |92        |  0|\n",
      "|1700954252|1275531832|72        |11        |  0|\n",
      "|1962509422|1689943334|297       |35        |  0|\n",
      "|1750363016|1295789907|548       |65        |  0|\n",
      "|1801891791|1215927538|50        |11        |  0|\n",
      "|1881694222|1467494161|157       |18        |  0|\n",
      "|1639152622|1295726032|1405      |249       |  0|\n",
      "|1548551070|1104846104|221       |12        |  0|\n",
      "|1972500924|1962580019|275       |16        |  0|\n",
      "+----------+----------+----------+----------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols=['_c3'], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_vector = vectorAssembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.3.17.29:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10a7af2b0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.3.17.29:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[4] appName=PySparkShell>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+----------+----------+--------+----------+\n",
      "|               Date|      Open|      High|       Low|     Close|  Volume| Adj Close|\n",
      "+-------------------+----------+----------+----------+----------+--------+----------+\n",
      "|2016-10-25 00:00:00|117.949997|118.360001|117.309998|    118.25|39190300|    118.25|\n",
      "|2016-10-24 00:00:00|117.099998|117.739998|     117.0|117.650002|23538700|117.650002|\n",
      "|2016-10-21 00:00:00|116.809998|116.910004|116.279999|116.599998|23192700|116.599998|\n",
      "|2016-10-20 00:00:00|116.860001|117.379997|116.330002|117.059998|24125800|117.059998|\n",
      "|2016-10-19 00:00:00|    117.25|117.760002|113.800003|117.120003|20034600|117.120003|\n",
      "+-------------------+----------+----------+----------+----------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = '../../../Galvanize/G65DS/DSI_Lectures/spark-aws/moses_marsh/data/aapl.csv'\n",
    "\n",
    "# read CSV# read  \n",
    "df_aapl = spark.read.csv(file_path,\n",
    "                         header=True,       # use headers or not\n",
    "                         quote='\"',         # char for quotes\n",
    "                         sep=\",\",           # char for separation\n",
    "                         inferSchema=True)  # do we infer schema or not ?\n",
    "\n",
    "df_aapl.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+--------------------+\n",
      "|      Open|      High|       Low|     Close|            features|\n",
      "+----------+----------+----------+----------+--------------------+\n",
      "|117.949997|118.360001|117.309998|    118.25|[117.949997,118.3...|\n",
      "|117.099998|117.739998|     117.0|117.650002|[117.099998,117.7...|\n",
      "|116.809998|116.910004|116.279999|116.599998|[116.809998,116.9...|\n",
      "|116.860001|117.379997|116.330002|117.059998|[116.860001,117.3...|\n",
      "|    117.25|117.760002|113.800003|117.120003|[117.25,117.76000...|\n",
      "+----------+----------+----------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "***************************************************************************\n",
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|[117.949997,118.3...|\n",
      "|[117.099998,117.7...|\n",
      "|[116.809998,116.9...|\n",
      "|[116.860001,117.3...|\n",
      "|[117.25,117.76000...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "***************************************************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([117.95, 118.36, 117.31, 118.25])),\n",
       " Row(features=DenseVector([117.1, 117.74, 117.0, 117.65])),\n",
       " Row(features=DenseVector([116.81, 116.91, 116.28, 116.6])),\n",
       " Row(features=DenseVector([116.86, 117.38, 116.33, 117.06])),\n",
       " Row(features=DenseVector([117.25, 117.76, 113.8, 117.12]))]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler\n",
    "\n",
    "# assemble values in a vector\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"Open\",\"High\", \"Low\",\"Close\"],\n",
    "                                  outputCol=\"features\")\n",
    "\n",
    "df_vector = vectorAssembler.transform(df_aapl)\n",
    "df_vector.select(['Open', 'High', 'Low', 'Close', 'features']).show(5)\n",
    "\n",
    "print(\"***\"*25)\n",
    "\n",
    "df_vector.select('features').show(5)\n",
    "\n",
    "print(\"***\"*25)\n",
    "\n",
    "df_vector.select('features').take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            features|      scaledfeatures|\n",
      "+--------------------+--------------------+\n",
      "|[117.949997,118.3...|[0.84364622791846...|\n",
      "|[117.099998,117.7...|[0.81798975110079...|\n",
      "|[116.809998,116.9...|[0.80923635459429...|\n",
      "|[116.860001,117.3...|[0.81074565144089...|\n",
      "|[117.25,117.76000...|[0.82251743035171...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "***************************************************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(scaledfeatures=DenseVector([0.8436, 0.8302, 0.8659, 0.866])),\n",
       " Row(scaledfeatures=DenseVector([0.818, 0.8109, 0.8563, 0.8473])),\n",
       " Row(scaledfeatures=DenseVector([0.8092, 0.7851, 0.8339, 0.8148])),\n",
       " Row(scaledfeatures=DenseVector([0.8107, 0.7997, 0.8355, 0.829])),\n",
       " Row(scaledfeatures=DenseVector([0.8225, 0.8115, 0.7568, 0.8309]))]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledfeatures\")\n",
    "\n",
    "# Compute summary statistics and generate MinMaxScalerModel\n",
    "scalerModel = scaler.fit(df_vector)\n",
    "\n",
    "# rescale each feature to range [min, max].\n",
    "scaledData = scalerModel.transform(df_vector)\n",
    "scaledData.select(\"features\", \"scaledfeatures\").show(5)\n",
    "\n",
    "print(\"***\"*25)\n",
    "\n",
    "scaledData.select(\"scaledfeatures\").take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "\n",
    "# Prepare training documents from a list of (id, text, label) tuples.\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a a a b b c a d spark\", 1.0),\n",
    "    (1, \"b c c c d c c a\", 0.0),\n",
    "    (2, \"spark spark a a c spam\", 1.0),\n",
    "    (3, \"c d d b d spam\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test documents, which are unlabeled (id, text) tuples.# Prepar \n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark a a a a\"),\n",
    "    (5, \"c c c p\"),\n",
    "    (6, \"spark spam spark a\"),\n",
    "    (7, \"a a a c c c\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# What do we need to do to this to get a prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.# Config \n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+\n",
      "|            features|prediction|         probability|\n",
      "+--------------------+----------+--------------------+\n",
      "|(262144,[227410,2...|       1.0|[4.25735553078518...|\n",
      "|(262144,[28698,21...|       0.0|[0.97099160578993...|\n",
      "|(262144,[197793,2...|       1.0|[0.00333910522987...|\n",
      "|(262144,[28698,22...|       1.0|[0.33293838014924...|\n",
      "+--------------------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#How can we test this against our training data?\n",
    "prediction = model.transform(test)\n",
    "prediction.select(['features', 'prediction', 'probability']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
